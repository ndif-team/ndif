# NDIF Ray Service

The **Ray service** is NDIF‚Äôs distributed execution and inference layer.  
It manages parallel workloads, model deployments, and inter-service coordination between NDIF‚Äôs API, queue, and telemetry systems.  
Built on [Ray](https://docs.ray.io/en/latest/), it provides a flexible and horizontally scalable runtime for distributed model execution and monitoring.

---

## üìò Overview

The Ray service powers NDIF‚Äôs distributed computation by:

- **Spawning and managing a Ray cluster** (head + workers) to handle distributed jobs.  
- **Deploying and scaling models** via Ray Serve and NDIF‚Äôs internal controller framework.  
- **Handling orchestration, evaluation, and scheduling**, including integrations like Google Calendar scheduling.  
- **Emitting structured logs and metrics** for centralized observability via Loki, InfluxDB, and Prometheus.  

It runs as one of NDIF‚Äôs main services (alongside `api`, `queue`, and telemetry containers) and can be started independently for testing.

---

## üìÅ Directory structure

src/services/ray/
‚îú‚îÄ‚îÄ environment.yml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ start.sh
‚îú‚îÄ‚îÄ start-worker.sh
‚îî‚îÄ‚îÄ src/
‚îú‚îÄ‚îÄ init.py
‚îú‚îÄ‚îÄ logging/ # Logging utilities (Loki/stdout shims)
‚îú‚îÄ‚îÄ metrics/ # Prometheus exporters and metric helpers
‚îú‚îÄ‚îÄ providers/ # External data providers (e.g., object store)
‚îú‚îÄ‚îÄ schema/ # Data schema definitions (Pydantic models)
‚îú‚îÄ‚îÄ types.py # Shared constants and enums
‚îî‚îÄ‚îÄ ray/
‚îú‚îÄ‚îÄ init.py
‚îú‚îÄ‚îÄ resources.py # Resource and device reporting utilities
‚îú‚îÄ‚îÄ config/
‚îÇ ‚îî‚îÄ‚îÄ ray_config.yml # Ray runtime configuration
‚îú‚îÄ‚îÄ deployments/
‚îÇ ‚îú‚îÄ‚îÄ init.py
‚îÇ ‚îú‚îÄ‚îÄ controller/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ init.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ controller.py # Orchestrates Ray Serve deployments
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ cluster/
‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ init.py
‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ cluster.py # Manages cluster state and scaling
‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ deployment.py # Deployment-level representation
‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ evaluator.py # Evaluation hooks and validation
‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ node.py # Node model (resources, identity, health)
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ gcal/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ init.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ controller.py # Calendar scheduling controller
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ scheduler.py # Scheduling logic (Google Calendar API)
‚îÇ ‚îî‚îÄ‚îÄ modeling/
‚îÇ ‚îú‚îÄ‚îÄ init.py
‚îÇ ‚îú‚îÄ‚îÄ base.py # Base model abstractions
‚îÇ ‚îú‚îÄ‚îÄ model.py # Model runner definitions
‚îÇ ‚îî‚îÄ‚îÄ util.py # Modeling utilities
‚îú‚îÄ‚îÄ distributed/
‚îÇ ‚îú‚îÄ‚îÄ init.py
‚îÇ ‚îú‚îÄ‚îÄ parallel_dims.py # Tensor and data parallel utilities
‚îÇ ‚îú‚îÄ‚îÄ util.py # Distributed execution helpers
‚îÇ ‚îî‚îÄ‚îÄ tensor_parallelism/
‚îÇ ‚îú‚îÄ‚îÄ init.py
‚îÇ ‚îú‚îÄ‚îÄ test.py # Test harness for tensor parallel plans
‚îÇ ‚îî‚îÄ‚îÄ plans/
‚îÇ ‚îú‚îÄ‚îÄ init.py
‚îÇ ‚îî‚îÄ‚îÄ llama.py # Tensor parallel plan for LLaMA-like models
‚îî‚îÄ‚îÄ nn/
‚îú‚îÄ‚îÄ init.py
‚îú‚îÄ‚îÄ backend.py # Execution backend abstraction
‚îú‚îÄ‚îÄ ops.py # Core NN ops distributed over Ray
‚îú‚îÄ‚îÄ sandbox.py # Experimental NN components
‚îî‚îÄ‚îÄ security/
‚îú‚îÄ‚îÄ init.py
‚îú‚îÄ‚îÄ protected_environment.py # Sandboxed exec environment
‚îî‚îÄ‚îÄ protected_object.py # Safe wrappers for model/data objects


---

## üß© Main classes and modules

| Component | Path | Description |
|------------|------|-------------|
| **Controller** | `ray/deployments/controller/controller.py` | Orchestrates Ray Serve deployments; handles start, scale, and teardown of NDIF workloads. |
| **Cluster** | `ray/deployments/controller/cluster/cluster.py` | Abstraction over Ray cluster lifecycle, node registration, and scaling logic. |
| **Node** | `ray/deployments/controller/cluster/node.py` | Represents an individual Ray node, including ID, resources, and health. |
| **Evaluator** | `ray/deployments/controller/cluster/evaluator.py` | Evaluates deployments and validates cluster configuration. |
| **Deployment** | `ray/deployments/controller/cluster/deployment.py` | Internal model describing deployment state and metadata. |
| **GCalController** | `ray/deployments/controller/gcal/controller.py` | Integrates Google Calendar scheduling for timed deployments or evaluations. |
| **GCalScheduler** | `ray/deployments/controller/gcal/scheduler.py` | Implements Google Calendar API logic and scheduling callbacks. |
| **Modeling** | `ray/deployments/modeling/*` | Defines base and derived model wrappers for Ray Serve tasks. |
| **Distributed utilities** | `ray/distributed/*` | Manages parallelism and tensor-parallel plans (especially `plans/llama.py`). |
| **NN backend and ops** | `ray/nn/backend.py`, `ray/nn/ops.py` | Provides neural network execution primitives under Ray. |
| **Protected environment** | `ray/nn/security/protected_environment.py` | Safeguards execution within sandboxed environments. |
| **Protected object** | `ray/nn/security/protected_object.py` | Wraps sensitive objects with restricted access controls. |

---

## ‚öôÔ∏è Dependencies (from `environment.yml`)

| Package | Purpose |
|----------|----------|
| `ray[serve]==2.47.0` | Core distributed compute and serving backend. |
| `prometheus_client` | Metric exporter for Grafana dashboards. |
| `python-logging-loki` | Loki log exporter (shimmed by `src/logging`). |
| `boto3` | Access to MinIO/S3 object stores. |
| `influxdb-client` | Write operational metrics to InfluxDB. |
| `google-api-python-client` | Integrates Google Calendar for scheduling. |
| `nnsight` | Used for NDIF model interpretability or inspection tasks (remove if unused). |
| `python-slugify` | Utility for slugging model or deployment names. |

> ‚ö†Ô∏è Remove the dangling `- google` entry at the bottom of `environment.yml` or replace it with specific Google libraries (`google-auth`, `google-auth-oauthlib`, etc.) actually imported in the source.

---

## üåç Environment variables (from NDIF Compose)

| Variable | Purpose |
|-----------|----------|
| `LOKI_URL` | URL for pushing logs to Loki. |
| `OBJECT_STORE_URL` | MinIO/S3 object store endpoint. |
| `API_URL` | URL of the NDIF API service. |
| `INFLUXDB_ADDRESS` / `INFLUXDB_*` | Metrics destination (InfluxDB connection, org, bucket, token). |
| `SCHEDULING_GOOGLE_CALENDAR_ID` | ID of the Google Calendar used for scheduling. |
| `SCHEDULING_GOOGLE_CREDS_PATH` | Path to the credentials file inside the Ray container. |
| `HOST_IP` | Host machine IP used to build service URLs. |
| `N_DEVICES` | Number of GPUs allocated to the Ray service container. |
| `RAY_DASHBOARD_HOST` | Bind address for the Ray Dashboard. |
| `RAY_METRICS_GAUGE_EXPORT_INTERVAL_MS` | Metric export interval (ms). |
| `RAY_SERVE_QUEUE_LENGTH_RESPONSE_DEADLINE_S` | Response timeout for Serve queue metrics. |

**Port mapping:**

| Service | Host Port | Container Port |
|----------|------------|----------------|
| Ray head | 6380 | 6379 |
| Ray client (`ray://`) | 9998 | 10001 |
| Ray dashboard | 8266 | 8265 |
| Ray Serve HTTP | 8267 | 8267 |

---

## üöÄ Spinning up the Ray service

### Option 1 ‚Äî via Docker Compose (recommended)

```bash
cd ndif/compose/dev
docker compose up ray

### Option 2 ‚Äî stand-alone (for development)

```bash
export $(grep -v '^#' compose/dev/.env | xargs)
python -m ray.src.main


‚ö†Ô∏è Without the API and queue services, the Ray container will run but cannot process NDIF workloads.

üß† Notes

The Ray service emits traces via OpenTelemetry and exposes metrics for Prometheus scraping.

Jaeger tracing identifies this service under Service = ray.

Logs flow to Grafana Loki with label {service="ray"}.

start-worker.sh is used to launch additional Ray workers from the same image when scaling horizontally.